# -*- coding: utf-8 -*-
"""BasicRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zhktfZy_janSnwKgDaWXRmm-E9OH7Br2
"""

# ! pip install -U openai langchain_community chromadb langchain sentence-transformers langchain_experimental -q

from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from openai import OpenAI  # This line was not changed
from langchain_core.prompts import PromptTemplate
import os
import dotenv

dotenv.load_dotenv()

embed_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

document = TextLoader("/Users/vinhvu/Downloads/after_batch_4.txt").load()

text_splitter = SemanticChunker(embed_model)
doc_splits = text_splitter.split_documents(document)
vectorstore = Chroma.from_documents(documents=doc_splits,
                                    embedding=embed_model,
                                    collection_name="local-rag")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

retriever = vectorstore.as_retriever(search_kwargs={"k":2})

def answer(question):
    """
    Retrieves documents based on the question, creates a RAG prompt, and gets the answer from the model.

    Parameters:
        question (str): The question that needs to be answered.
        retriever: The retrieval component used to get relevant documents.
        client: The client for interacting with the language model.

    Returns:
        str: The answer generated by the language model.
    """

    template = """
    You are a knowledgeable assistant capable of answering complex questions by utilizing the provided information from retrieved documents.\n Context:\n {context} \n Question:\n {question} \n Answer:
    """
    rag_prompt = PromptTemplate.from_template(template)
    documents = retriever.invoke(question)
    context = "\n".join([doc.page_content for doc in documents])
    formatted_prompt = rag_prompt.format(question=question, context=context)
    completion = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": formatted_prompt}
        ]
    )

    answer = completion.choices[0].message.content
    return answer

# question = "What is the main content of the document?"
# answer = answer(question)
# print(answer)

import chainlit as cl


@cl.step(type="tool")
async def tool(message):
    # Fake tool
    await cl.sleep(2)
    response = answer(message)
    return response
    return "Response from the tool!"


@cl.on_message  # this function will be called every time a user inputs a message in the UI
async def main(message: cl.Message):
    """
    This function is called every time a user inputs a message in the UI.
    It sends back an intermediate response from the tool, followed by the final answer.

    Args:
        message: The user's message.

    Returns:
        None.
    """

    final_answer = await cl.Message(content="").send()

    # Call the tool
    final_answer.content = await tool(message.content)

    await final_answer.update()